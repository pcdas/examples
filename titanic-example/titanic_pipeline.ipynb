{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic dataset Training Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the notebook end-to-end\n",
    "* Setting ```skip_test_code``` variable to ```True``` skips unnecessary code execution,\n",
    "  enabling execution of the notebook in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_test_code = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Docker container image that will run pipeline steps\n",
    "* A new image is required because the default image does not contain these python modules: pandas, sklearn\n",
    "* The docker image is built once using Kaniko inside the k8s cluster and pushed to a docker repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_docker_image(image_name: str, docker_registry: str) -> str:\n",
    "    \"\"\"Builds the docker image, pushes to the registry, and returns the full usable image tag.\"\"\"\n",
    "    # from kubeflow.fairing import constants\n",
    "    # print(constants.constants.KANIKO_IMAGE)\n",
    "    from kubeflow.fairing.preprocessors import base as base_preprocessor\n",
    "    from kubeflow.fairing.builders import cluster\n",
    "    from kubeflow.fairing.builders.cluster.minio_context import MinioContextSource\n",
    "    import logging\n",
    "\n",
    "    # output_map is a map of extra files to add to the notebook.\n",
    "    # It is a map from source location to the location inside the context.\n",
    "    output_map =  {\n",
    "        \"Dockerfile.pipeline\": \"Dockerfile\"\n",
    "    }\n",
    "\n",
    "    preprocessor = base_preprocessor.BasePreProcessor(\n",
    "        command=[\"python\"], # The base class will set this.\n",
    "        input_files=[],\n",
    "        path_prefix=\"/app\", # irrelevant since we aren't preprocessing any files\n",
    "        output_map=output_map)\n",
    "\n",
    "    preprocessor.preprocess()\n",
    "\n",
    "    minio_context_source = MinioContextSource(endpoint_url=minio_endpoint, minio_secret=minio_username,\n",
    "                                              minio_secret_key=minio_key, region_name=minio_region)\n",
    "    cluster_builder = cluster.cluster.ClusterBuilder(registry=docker_registry,\n",
    "                                                     base_image=\"\", # base_image is set in the Dockerfile\n",
    "                                                     preprocessor=preprocessor,\n",
    "                                                     image_name=image_name,\n",
    "                                                     dockerfile_path=\"Dockerfile\",\n",
    "                                                     context_source=minio_context_source)\n",
    "    cluster_builder.build()\n",
    "    logging.info(f\"Built image {cluster_builder.image_tag}\")\n",
    "    return cluster_builder.image_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image needs to be built, but only once\n",
    "if not skip_test_code:\n",
    "    image_tag = build_docker_image('titanic-pipeline', 'pcdas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python packages when container runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "from typing import List\n",
    "def install_packages(packages_to_install: List[str]) -> str:\n",
    "    import sys\n",
    "    import subprocess\n",
    "    for pkg in packages_to_install:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "    return 'installed: ' + str(packages_to_install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Minio/S3 global config & auth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minio_config() -> dict:\n",
    "    return {\n",
    "        \"endpoint\":\"http://minio-service.kubeflow.svc.cluster.local:9000\",\n",
    "        \"username\":\"minio\",\n",
    "        \"key\":\"minio123\",\n",
    "        \"region\":\"us-east-1\"\n",
    "    }    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Minio/S3 file into the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_to(file_name: str, bucket_name: str, uploaded_file_name: str):\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    minio = get_minio_config()\n",
    "    session = boto3.session.Session()\n",
    "    s3 = boto3.client('s3', endpoint_url=minio['endpoint'], aws_access_key_id=minio['username'], \n",
    "                      aws_secret_access_key=minio['key'], config=Config(signature_version='s3v4'),\n",
    "                      region_name=minio['region'], use_ssl=False)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        s3.download_fileobj(bucket_name, uploaded_file_name, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the prepared file to Minio/S3 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to(file_name: str, bucket_name: str, prepared_file: str):\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    minio = get_minio_config()\n",
    "    session = boto3.session.Session()\n",
    "    s3 = boto3.client('s3', endpoint_url=minio['endpoint'], aws_access_key_id=minio['username'], \n",
    "                      aws_secret_access_key=minio['key'], config=Config(signature_version='s3v4'),\n",
    "                      region_name=minio['region'], use_ssl=False)\n",
    "    with open(prepared_file, 'rb') as f:\n",
    "        s3.upload_fileobj(f, bucket_name, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Titanic dataset from Minio/S3 and create a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def create_dataframe(bucket_name: str, uploaded_file_name: str) -> pd.DataFrame:\n",
    "    downloaded_file_name = '/tmp/' + uploaded_file_name\n",
    "    download_file_to(downloaded_file_name, bucket_name, uploaded_file_name)\n",
    "    dft = pd.read_csv(downloaded_file_name)\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "if not skip_test_code:\n",
    "    dft = create_dataframe('pcdas', 'titanic.csv')\n",
    "    print(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "if not skip_test_code:\n",
    "    sns.countplot(dft['Sex'], hue=dft['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def do_dataprep1(dft: pd.DataFrame):\n",
    "    import numpy as np\n",
    "    dft['familySize'] = dft['SibSp'] + dft['Parch'] + 1\n",
    "\n",
    "    # fill null values\n",
    "    dft['Cabin'] = dft['Cabin'].fillna('U')\n",
    "\n",
    "    # fill null value with mode for the column\n",
    "    dft['Embarked'] = dft['Embarked'].fillna(dft[\"Embarked\"].mode()[0])\n",
    "\n",
    "    # change values to numbers\n",
    "    dft['Sex'] = np.where(dft['Sex'] == 'male', 0,\n",
    "                 np.where(dft['Sex'] == 'female', 1, -1))\n",
    "\n",
    "    dft['Embarked'] = np.where(dft[\"Embarked\"] == \"S\", 0,\n",
    "                      np.where(dft[\"Embarked\"] == \"C\", 1,\n",
    "                      np.where(dft[\"Embarked\"] == \"Q\", 2, -1)))\n",
    "\n",
    "    # convert age to integer values after replacing null values with -1\n",
    "    dft['Age'] = dft['Age'].fillna(-1)\n",
    "    dft['Age'] = dft['Age'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "if not skip_test_code:\n",
    "    do_dataprep1(dft)\n",
    "    dft.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep 2 (Add Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# fill null ages and Fares with median values\n",
    "def fillna_withdict(df: pd.DataFrame, col: str, acol: str, dict1: dict) -> pd.DataFrame: \n",
    "    import numpy as np\n",
    "    print(dict1)\n",
    "    for aggcol in dict1:\n",
    "        val = dict1[aggcol]\n",
    "        df[col] = np.where(((df[col].isna()) & (df[acol] == aggcol)), val, df[col])\n",
    "    return df\n",
    "\n",
    "def do_dataprep2(dft: pd.DataFrame) -> pd.DataFrame:\n",
    "    dict1 = dft.groupby(\"Pclass\")['Age'].median().to_dict()  \n",
    "    dft = fillna_withdict(dft,'Age','Pclass', dict1)\n",
    "\n",
    "    dict1 = dft.groupby(\"Pclass\")['Fare'].median().to_dict()  \n",
    "    dft = fillna_withdict(dft, 'Fare', 'Pclass', dict1)\n",
    "\n",
    "    # convert cabin to numbers\n",
    "    dft['v_Cabin'] = dft['Cabin'].str[0]\n",
    "\n",
    "    cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\n",
    "    dft['Cabintype'] = dft['v_Cabin'].map(cabin_category)\n",
    "\n",
    "    # fills nulls for Embarked with 0\n",
    "    dft[\"Embarked\"] = dft[\"Embarked\"].fillna(0)\n",
    "\n",
    "    # drop redundant and other text/string columns\n",
    "    dft = dft.drop(['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin','v_Cabin'], axis = 1)\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "if not skip_test_code:\n",
    "    dft = do_dataprep2(dft)\n",
    "    dft.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_test_code:\n",
    "    sns.countplot(dft['Pclass'], hue=dft['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write prepared dataframe to a local file system and upload to Minio/S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def write_dataframe_and_upload(dft: pd.DataFrame, bucket_name: str, file_name: str) -> str:\n",
    "    prepared_file='/tmp/' + file_name\n",
    "    dft.to_csv(prepared_file, sep=',', index=False)\n",
    "    upload_file_to(file_name, bucket_name, prepared_file)\n",
    "    return bucket_name + '/' + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "if not skip_test_code:\n",
    "    uploaded_fullpath = write_dataframe_and_upload(dft, 'pcdas', 'titan2.csv')\n",
    "    print(f\"Minio path is {uploaded_fullpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ML Training Pipeline\n",
    "* Evaluate Linear Regression and Random Forest models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline step for data preparation\n",
    "* Data preparation task will be run in a container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dataprep(uploaded_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes in uploaded file to process. After preparation using a local filesystem,\n",
    "    returns the uploaded prepared filename.\n",
    "    \"\"\"\n",
    "    parts = uploaded_file.split('/')\n",
    "    bucket = parts[0]\n",
    "    file_name = parts[1]\n",
    "    dft = create_dataframe(bucket, file_name)\n",
    "    do_dataprep1(dft)\n",
    "    dft = do_dataprep2(dft)\n",
    "    \n",
    "    parts = file_name.split('.')\n",
    "    prepared_file_name = parts[0] + '_prepared.' + parts[1]\n",
    "    uploaded_fullpath = write_dataframe_and_upload(dft, bucket, prepared_file_name)\n",
    "    return uploaded_fullpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "if not skip_test_code:\n",
    "    uploaded_file = do_dataprep('pcdas/titanic.csv')\n",
    "    print(f'Uploaded file to {uploaded_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "def sklearn_Linear_Regression(prepared_file_name: str) -> str:\n",
    "    logging.info(f'Prepared filename: {prepared_file_name}')\n",
    "    parts = prepared_file_name.split('/')\n",
    "    dft = create_dataframe(parts[0], parts[1])\n",
    "    return sklearn_Linear_Regression_1(dft)\n",
    "\n",
    "def sklearn_Linear_Regression_1(train: pd.DataFrame) -> str:\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Here is out local validation scheme!\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived', 'PassengerId'], axis = 1), \n",
    "                                                        train['Survived'], test_size = 0.2,\n",
    "                                                        random_state = None)\n",
    "\n",
    "    # We'll use a logistic regression model again, but we'll go to something more fancy soon! \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    logisticRegression = LogisticRegression(max_iter = 10000)\n",
    "    logisticRegression.fit(X_train, y_train)\n",
    "\n",
    "    # Predict!\n",
    "    predictions = logisticRegression.predict(X_test)\n",
    "\n",
    "    # Print our preditions\n",
    "    print(predictions)\n",
    "    # Check mean\n",
    "    round(np.mean(predictions), 2)\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    # Print the resulting confusion matrix\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Calculate Accuracy!\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    # Set our robust cross-validation scheme!\n",
    "    kf = KFold(n_splits = 5, random_state = 2)\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    # Print our CV accuracy estimate:\n",
    "    acc1 = cross_val_score(logisticRegression, X_test, y_test, cv = kf).mean() \n",
    "    return str(acc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "if not skip_test_code:\n",
    "    accuracy_result1 = sklearn_Linear_Regression('pcdas/titanic_prepared.csv')\n",
    "    print(accuracy_result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "def random_forest(prepared_file_name: str) -> str:\n",
    "    logging.info(f'Prepared filename: {prepared_file_name}')\n",
    "    parts = prepared_file_name.split('/')\n",
    "    dft = create_dataframe(parts[0], parts[1])\n",
    "    return random_forest_1(dft)\n",
    "\n",
    "def random_forest_1(train: pd.DataFrame) -> str:\n",
    "    import sklearn\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import numpy as np \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Here is out local validation scheme!\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived', 'PassengerId'], axis = 1), \n",
    "                                                        train['Survived'], test_size = 0.2,\n",
    "                                                        random_state = None)\n",
    "\n",
    "    #Initialize randomForest\n",
    "    randomForest = RandomForestClassifier(random_state = 2)\n",
    "\n",
    "    # Set our parameter grid\n",
    "    param_grid = { \n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_features': ['auto', 'log2'],\n",
    "        'max_depth' : [3, 5, 7]    \n",
    "    }\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # Grid search\n",
    "    randomForest_CV = GridSearchCV(estimator = randomForest, param_grid = param_grid, cv = 5)\n",
    "    randomForest_CV.fit(X_train, y_train)\n",
    "\n",
    "    # Print best hyperparameters\n",
    "    randomForest_CV.best_params_\n",
    "    randomForestFinalModel = RandomForestClassifier(random_state = 2, criterion = 'gini', max_depth = 7, max_features = 'auto', n_estimators = 300)\n",
    "    \n",
    "    # Fit the model to the training set\n",
    "    randomForestFinalModel.fit(X_train, y_train)\n",
    "    # Predict!\n",
    "    predictions = randomForestFinalModel.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Calculate the accuracy for our powerful random forest!\n",
    "    acc2 = round(accuracy_score(y_test, predictions), 2)\n",
    "    print(\"accuracy is: \", round(accuracy_score(y_test, predictions), 2))   \n",
    "    return str(acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect accuracy results from both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_step(acc1: str, acc2: str) -> str:\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    metadata = {\n",
    "        'outputs' : [\n",
    "            {\n",
    "              'storage': 'inline',\n",
    "              'source': '# LR Accuracy\\n' + acc1,\n",
    "              'type': 'markdown',\n",
    "            },\n",
    "            {\n",
    "              'storage': 'inline',\n",
    "              'source': '# RF Accuracy \\n' + acc2,\n",
    "              'type': 'markdown',\n",
    "            }        \n",
    "        ]\n",
    "    }\n",
    "    return str(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and compile the traning pipeline\n",
    "* Uses a pre-built docker image that contains installed modules: pandas, sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_tag() -> str:\n",
    "    return 'pcdas/titanic-pipeline:4899B81E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "\n",
    "# converting functions to container operation\n",
    "image_tag = get_image_tag()\n",
    "dataprep_operation = func_to_container_op(do_dataprep, base_image=image_tag, use_code_pickling=True)\n",
    "sklearn_operation = func_to_container_op(sklearn_Linear_Regression,\n",
    "                                         base_image=image_tag, use_code_pickling=True)\n",
    "rforest_operation = func_to_container_op(random_forest,\n",
    "                                         base_image=image_tag, use_code_pickling=True)\n",
    "final_step_operation = func_to_container_op(final_step)\n",
    "\n",
    "from kfp.dsl import pipeline \n",
    "@pipeline( # defining pipeline metadata\n",
    "    name='titanic-prep-training',\n",
    "    description='Prepare titanic dataset'\n",
    ")\n",
    "# stitch the pipeline steps\n",
    "def dataprep_training_pipeline(context: str = \"pcdas/titanic.csv\"):\n",
    "    import logging\n",
    "    step_0 = dataprep_operation(context)\n",
    "    logging.info(step_0.output)\n",
    "    step_1_1 = sklearn_operation(step_0.output) \n",
    "    step_1_2 = rforest_operation(step_0.output) \n",
    "    step_3 = final_step_operation(step_1_1.output, step_1_2.output) \n",
    "    \n",
    "from kfp.compiler import Compiler\n",
    "# generate the pipeline file that can be executed\n",
    "pipeline_file = 'titanic-pipeline.zip'\n",
    "Compiler().compile(dataprep_training_pipeline, pipeline_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import Client\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(name = \"Titanic dataset Experiment2\") #creating experiment\n",
    "run_name = \"Titanic ML Test2\"\n",
    "pipeline_filename = pipeline_file\n",
    "run_result = client.run_pipeline( # submit a pipeline run to k8s\n",
    "    experiment.id, \n",
    "    run_name,\n",
    "    pipeline_filename,\n",
    "    params = {}\n",
    ")\n",
    "print(run_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
